---
title: "Reproducibility review of: Extracting interrogative intents and concepts from geo-analytic questions"
author: "Daniel Nüst \\orcid{0000-0002-0024-5046}"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  pdf_document:
    toc: false
header-includes:
  - |
    % https://tex.stackexchange.com/questions/445563/ieeetran-how-to-include-orcid-in-tex-pdf-with-pdflatex/445583 (works with pdflatex)
    \usepackage{scalerel}
    \usepackage{tikz}
    \usetikzlibrary{svg.path}
    \definecolor{orcidlogocol}{HTML}{A6CE39}
    \tikzset{
      orcidlogo/.pic={
        \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3    ,256,128z};
        \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
                     svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z     M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
                     svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88    .7,51.3,88.7,56.8z};
      }
    }
    \newcommand\orcid[1]{\href{https://orcid.org/#1}{\raisebox{0.15 em}{\mbox{\scalerel*{
    \begin{tikzpicture}[yscale=-1, transform shape]
    \pic{orcidlogo};
    \end{tikzpicture}
    }{|}}}}}
    \definecolor{agileblue}{RGB}{0,77,155}
urlcolor: agileblue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r logo, echo = FALSE, message=FALSE, fig.align='center', out.width='0.3\\linewidth', fig.pos='H'}
temp <- tempfile(fileext = ".pdf")
download.file(url = "https://github.com/reproducible-agile/reproducible-agile.github.io/raw/master/public/images/reproducible-AGILE-logo-square.pdf", destfile = temp)
knitr::include_graphics(temp)
```

This report is part of the reproducibility review at the AGILE conference.
For more information see [https://reproducible-agile.github.io/](https://reproducible-agile.github.io/).
This document is published on OSF at [https://osf.io/7xrqg/](https://osf.io/7xrqg/).
To cite the report use

>  _Nüst, Daniel (2020). Reproducibility review of: Extracting interrogative intents and concepts from geo-analytic questions. https://doi.org/10.17605/OSF.IO/7XRQG _

# Reviewed paper

> _Haiqi Xu, Ehsan Hamzei, Enkhbold Nyamsuren, Han Kruiger, Stephan Winter, Martin Tomko and Simon Scheider: Extracting interrogative intents and concepts from geo-analytic questions. AGILE GiScience Ser., 1, 22. https://doi.org/10.5194/agile-giss-1-22-2020, 2020._

# Summary

The paper code and a sample dataset were published in an anonymous Figshare record at https://figshare.com/s/b3f8b0834ca63b6c5d60 under a Creative Commons BY-NC-ND 4.0 license.
I could execute the workflow without errors following the provided instructions.
The scripts created a subset of the figures only, _some key figures were not created by the provided data and code_.
The authors show good concern for transparency and reproducibility and this _reproduction was partially successful_.

```{r, echo=FALSE, eval=FALSE, results='hide'}
# create ZIP of reproduction files and upload to OSF
library("zip")
library("here")

zipfile <- here::here("2020-018/agile-reproreview-2020-018.zip")
file.remove(zipfile)
zip::zipr(zipfile,
          here::here("2020-018/11407371/GeoParser-AGILE2020/"))

library("osfr") # See docs at https://docs.ropensci.org/osfr/
# OSF_PAT is in .Renviron in parent directory
# We cannot use osfr to create a new component (with osfr::osf_create_component(x = osfr::osf_retrieve_node("6k5fh"), ...) because that will set the storage location to outside Europe.

# retrieve project
project <- osfr::osf_retrieve_node("7xrqg")

# upload files
osfr::osf_upload(x = project,
                 conflicts = "overwrite",
                 path = c(list.files(here("2020-018"), pattern = "agile-reproreview-.*(pdf$|Rmd$|zip$)"), "COPYRIGHT")
                 )
```
\clearpage

# Reproducibility reviewer notes

The paper contains an a Data and Software Availability section and a link to an anonymous Figshare record at https://figshare.com/s/b3f8b0834ca63b6c5d60 published under a CC-BY-NC-ND 4.0 license, which does not match the licensing information in the README ("MIT license").

All scientific reviewers took note of the repository, but did not attempt execution or reproduction.
I skimmed the article briefly and then downloaded the archive from Figshare and continued with the succinct `readme.md`.
The author kindly reported the expected execution time ("10 hours"), so I created a virtual environment and started the workflow.

The environment uses Python 3.7 (not 3.6 as reported by the authors).
The review repository contains a `Pipfile.lock` describing the used environment in detail, and was created with the following commands:

```bash
pipenv --python 3.7
pipenv install xlsxwriter matplotlib numpy scipy pandas sklearn wordcloud torch allennlp
pipenv lock
pipenv lock -r > requirements.txt
```

## Parsing

```bash
$ pipenv shell

# cd 11407371/GeoParser-AGILE2020/ > Figshare's ZIP, GeoParser-AGILE2020.zip
$ python parse.py
```

Code ran overnight and the final log lines and files created in the directory `parsing_result` are below.

```
[...]
INFO:root:*******************************************************
INFO:root:Processing the record number ::: 24559; Currently 100.0% of the MS MARCO is parsed
INFO:allennlp.models.archival:removing temporary unarchived model dir at /tmp/tmpgtqmz3z3
INFO:allennlp.models.archival:removing temporary unarchived model dir at /tmp/tmpnmwgsmfe

$ ll -h parsing_result/
total 23M
drwxrwxr-x 2 daniel daniel 4,0K Jun  5 04:33  ./
drwxrwxr-x 9 daniel daniel 4,0K Dez 23 00:47  ../
-rw-r--r-- 1 daniel daniel 698K Jun  4 23:26  GeoAnQu.json
-rw-r--r-- 1 daniel daniel 209K Jun  4 23:29  GeoQuestion201.json
-rw-r--r-- 1 daniel daniel  22M Jun  5 04:33 'MS MARCO.json'
```

That seems like a successful execution of that script.

## Visualisation

```bash
$ python visualization.py

INFO:allennlp.modules.elmo:Initializing ELMo
INFO:root:analyzing GeoQuestion201...
INFO:root:analyzing GeoAnQu...
INFO:root:analyzing MS MARCO...
INFO:root:Comparing datasets (random subset: 200, dimension: 17d)...
INFO:root:Comparing datasets (all)...

$ ll -h graphs/
total 19M
drwxrwxr-x 2 daniel daniel 4,0K Jun  5 08:04 ./
drwxrwxr-x 9 daniel daniel 4,0K Dez 23 00:47 ../
-rw-r--r-- 1 daniel daniel 149K Jun  5 08:02 2d_encoding_comparison.png
-rw-r--r-- 1 daniel daniel 224K Jun  5 08:04 2d_encoding_comparison-without-sampling.png
-rw-r--r-- 1 daniel daniel 471K Jun  5 08:02 GeoAnQu_activities.png
-rw-r--r-- 1 daniel daniel 762K Jun  5 08:02 GeoAnQu_objects.png
-rw-r--r-- 1 daniel daniel 710K Jun  5 08:02 GeoAnQu_ointents.png
-rw-r--r-- 1 daniel daniel 744K Jun  5 08:02 GeoAnQu_oqualities.png
-rw-r--r-- 1 daniel daniel 714K Jun  5 08:01 GeoAnQu_pnames.png
-rw-r--r-- 1 daniel daniel 778K Jun  5 08:01 GeoAnQu_ptypes.png
-rw-r--r-- 1 daniel daniel 728K Jun  5 08:02 GeoAnQu_qualities.png
-rw-r--r-- 1 daniel daniel 750K Jun  5 08:02 GeoAnQu_questions.png
-rw-r--r-- 1 daniel daniel 694K Jun  5 08:02 GeoAnQu_situations.png
-rw-r--r-- 1 daniel daniel 667K Jun  5 08:02 GeoAnQu_tintents.png
-rw-r--r-- 1 daniel daniel  95K Jun  5 08:01 GeoQuestion201_activities.png
-rw-r--r-- 1 daniel daniel 619K Jun  5 08:01 GeoQuestion201_objects.png
-rw-r--r-- 1 daniel daniel 340K Jun  5 08:01 GeoQuestion201_ointents.png
-rw-r--r-- 1 daniel daniel 362K Jun  5 08:01 GeoQuestion201_oqualities.png
-rw-r--r-- 1 daniel daniel 713K Jun  5 08:01 GeoQuestion201_pnames.png
-rw-r--r-- 1 daniel daniel 575K Jun  5 08:01 GeoQuestion201_ptypes.png
-rw-r--r-- 1 daniel daniel 478K Jun  5 08:01 GeoQuestion201_qualities.png
-rw-r--r-- 1 daniel daniel 718K Jun  5 08:01 GeoQuestion201_questions.png
-rw-r--r-- 1 daniel daniel 201K Jun  5 08:01 GeoQuestion201_situations.png
-rw-r--r-- 1 daniel daniel 510K Jun  5 08:01 GeoQuestion201_tintents.png
-rw-r--r-- 1 daniel daniel 746K Jun  5 08:02 MSMARCO_activities.png
-rw-r--r-- 1 daniel daniel 777K Jun  5 08:02 MSMARCO_objects.png
-rw-r--r-- 1 daniel daniel 792K Jun  5 08:02 MSMARCO_ointents.png
-rw-r--r-- 1 daniel daniel 740K Jun  5 08:02 MSMARCO_oqualities.png
-rw-r--r-- 1 daniel daniel 782K Jun  5 08:02 MSMARCO_pnames.png
-rw-r--r-- 1 daniel daniel 446K Jun  5 08:02 MSMARCO_ptypes.png
-rw-r--r-- 1 daniel daniel 819K Jun  5 08:02 MSMARCO_qualities.png
-rw-r--r-- 1 daniel daniel 804K Jun  5 08:02 MSMARCO_questions.png
-rw-r--r-- 1 daniel daniel 436K Jun  5 08:02 MSMARCO_situations.png
-rw-r--r-- 1 daniel daniel 426K Jun  5 08:02 MSMARCO_tintents.png
```

The `graphs` directory now contains a number of plots, some of which I could match to Figures in the paper: `GeoQuestion201_objects.png` to **Figure 6 (a), `GeoQuestion201_situations.png` seems to be Figure 7 (c), but with the additional word "flows", `GeoQuestion201_activities.png` matches Figure 7 (d), `2d_encoding_comparison.png` seems to match Figure 10 in content, but not in the shown data values.
For some figures there seems to be no match in the paper, e.g., `2d_encoding_comparison-without-sampling.png`.

**This part of the workflow seems reproducible, the differences in Figures are possibly due to randomness effects.**
Some figures were not included in the reproductions, e.g. Figures 3, 4, 5 (a), 8, 9, 11, 12, 13.

### Reproduced Figure 6 (a)

![](11407371/GeoParser-AGILE2020/graphs/GeoQuestion201_objects.png)

### Reproduced Figure 7 (d)

![](11407371/GeoParser-AGILE2020/graphs/GeoQuestion201_activities.png)

### Figure 10

![](11407371/GeoParser-AGILE2020/graphs/2d_encoding_comparison.png)

## Evaluation script

```bash
$ cd evaluation
$ python evaluation.py
[...]
$ ll -h
total 180K
drwxrwxr-x 2 daniel daniel 4,0K Jun  5 09:05 ./
drwxrwxr-x 9 daniel daniel 4,0K Dez 23 00:47 ../
-rw-rw-r-- 1 daniel daniel  22K Dez 23 00:47 1.csv
-rw-rw-r-- 1 daniel daniel  21K Dez 23 00:47 2.csv
-rw-rw-r-- 1 daniel daniel  22K Dez 23 00:47 3.csv
-rw-rw-r-- 1 daniel daniel  21K Dez 23 00:47 4.csv
-rw-r--r-- 1 daniel daniel 8,4K Jun  5 09:05 a1-questions.txt
-rw-r--r-- 1 daniel daniel 8,2K Jun  5 09:05 a2-questions.txt
-rw-r--r-- 1 daniel daniel 8,5K Jun  5 09:05 a3-questions.txt
-rw-r--r-- 1 daniel daniel 8,1K Jun  5 09:05 a4-questions.txt
-rw-r--r-- 1 daniel daniel  148 Jun  5 09:05 errors.txt
-rw-rw-r-- 1 daniel daniel  22K Dez 23 00:47 evaluation.py
```

Again, this part of the workflow seems to execute without error, but it's unclear what information these files provide to the article.

------

## Comments to the authors

I briefly skimmed over the script files, mostly to identify the expected outputs.
I find the files readable and well formatted.
The file names of most data/input/output files are understandable with the background of the paper.
The private Figshare project is very well suited for peer review but should be made public and linked to via a DOI for the final publication.

Overall: good job, it was a good experience to reproduce the workflow.
I have the following concrete recommendations:

- Reconsider the very restrictive license for your own code and data.
- Better connect created plot names with the figures of the article, e.g., by matching Figure numbers (though I understand that can be tedious) or matching file name and the figure text. Also, consider not creating plots that are not found in the paper at all.
- The list of required libraries is good, you could do even better with a ready-to-use environment spec, e.g., using Pipenv or Conda.
- For higher reproducibility, maybe you can provide files needed within the repository, e.g., when running `parse.py` several logs mention downloading of (model?) files, e.g.,
  ```
  INFO:allennlp.common.file_utils:https://allennlp.s3.amazonaws.com/models/
    fine-grained-ner-model-elmo-2018.12.21.tar.gz not found in cache, downloading to ...
  ```
  which could result in broken workflows if the downloads are not available anymore.
- Document in the README which files are generated by which script.
- Consider setting a seed so that the reproduction creates precisely the same wordcloud.
- There is a function `read_dummy_samples` in `parse.py` > consider providing a synthetic dataset for quicker demonstration of your workflow, together with a small set of expected output files to compare against.
